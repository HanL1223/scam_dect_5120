{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required dependencies\n",
    "import pandas as pd\n",
    "from transformers import BertForPreTraining, BertTokenizer, AutoTokenizer, AutoModelForSequenceClassification, DistilBertConfig,BertForSequenceClassification, AutoModelForSequenceClassification, AutoTokenizer, get_scheduler\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve,confusion_matrix\n",
    "import optuna\n",
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu' )\n",
    "data_path = 'full_Data.csv'\n",
    "#device = torch.device(\"mps\" if torch.backends.mps.is_available() and device != 'cuda' else 'cpu' )\n",
    "df = pd.read_csv(data_path,index_col= [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove na and duplicate\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in an SMS: 1259.6406191879596\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['num_words'] = df.Text.apply(len)\n",
    "sns.displot(df.num_words)\n",
    "plt.xlim(0, 3000)\n",
    "print(\"Average number of words in an SMS:\",df.num_words.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18703</th>\n",
       "      <td>enron : a wake - up call\\nthe wall street jour...</td>\n",
       "      <td>ham</td>\n",
       "      <td>228353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18374</th>\n",
       "      <td>rival to buy enron , top energy trader , after...</td>\n",
       "      <td>ham</td>\n",
       "      <td>178837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18640</th>\n",
       "      <td>fall of a power giant : bailout is unlikely if...</td>\n",
       "      <td>ham</td>\n",
       "      <td>165933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18578</th>\n",
       "      <td>accounting peer review gets more scrutiny\\nthe...</td>\n",
       "      <td>ham</td>\n",
       "      <td>141870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18600</th>\n",
       "      <td>enron and dynegy discuss plan to cut price of ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>136478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16483</th>\n",
       "      <td>thankyou</td>\n",
       "      <td>ham</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Thank u!</td>\n",
       "      <td>ham</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>My phone</td>\n",
       "      <td>ham</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Thanx...</td>\n",
       "      <td>ham</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32066</th>\n",
       "      <td>un 2 u 6</td>\n",
       "      <td>spam</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34899 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Label  num_words\n",
       "18703  enron : a wake - up call\\nthe wall street jour...   ham     228353\n",
       "18374  rival to buy enron , top energy trader , after...   ham     178837\n",
       "18640  fall of a power giant : bailout is unlikely if...   ham     165933\n",
       "18578  accounting peer review gets more scrutiny\\nthe...   ham     141870\n",
       "18600  enron and dynegy discuss plan to cut price of ...   ham     136478\n",
       "...                                                  ...   ...        ...\n",
       "16483                                           thankyou   ham          8\n",
       "619                                             Thank u!   ham          8\n",
       "3256                                            My phone   ham          8\n",
       "270                                             Thanx...   ham          8\n",
       "32066                                           un 2 u 6  spam          8\n",
       "\n",
       "[34899 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[df['num_words'] < 8].index,inplace = True)\n",
    "df = df.reset_index(drop = True)\n",
    "df.sort_values(by ='num_words',ascending = False) # The longest sms seems to be a news clip, we also want to inspect what is people sening around 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attached below is the cpuc \\' s proposed decision in the gas industry\\nrestructuring proceeding . essentially , it adopts the interim proposal with a\\nfew modifications . the cpuc states that they are taking a \" cautious \"\\napproach to deregulation in light of recent market events in both electricity\\nand gas , and therefore , rejected the more robust reforms outlined in the\\ncomprehensive settlement . the cs was offered by socal gas and supported by\\nmany of the settlement parties , including enron and transwestern .\\nthe good news is that , with respect to hector road , the maximum volume at\\nhector is limited to 50 mmcf / d , similar to the proposal made by the\\ncomprehensive settlement . i \\' ve included the section from the pd that deals\\nwith this issue for your convenience . the full document is also attached .\\nexcerpt at at page 50 regarding hector road :\\n( 1 ) receipt points / intrastate transmission\\nas we have already discussed above , we now judge that intrastate transmission\\nunbundling is not wise at this time for the socalgas system . however ,\\nsocalgas \\x01 , intrastate transmission system can still be made more accessible\\nand understandable to its users .\\nin r . 98 - 01 - 011 , the record reflects dissatisfaction among customers and\\nshippers with the lack of clarity on how socalgas schedules gas shipments\\nthrough its windowing system , and socalgas \\x01 , sole use of the hector road\\ninterconnection as a receipt point . ( exh . 8 in r . 98 - 01 - 011 , pp . 29 - 31\\n( southern california edison company market conditions report ) , ( panel hearing\\ntestimony of mr . paul carpenter , southern california edison , tr . pp .\\n931 - 932 , jan 25 , 1999 ) . ) our decision in d . 99 - 07 - 015 directed investigation\\ninto using the hector road interconnection , even on an interim basis , and the\\npublication of socalgas \\x01 , windowing criteria in tariffs . socalgas filed\\nadvice letter 2837 , which detailed its process of basing a maximum amount of\\ngas scheduled for shipment through a receipt point on the prior day \\x01 , s\\nnominations , except at the first of the month . early in the instant\\nproceeding , the alj held in abeyance active consideration of the windowing\\nprocedure tariff socalgas filed , pending the resolution we reach today .\\n( prehearing conference of september 1 , 1999 , p . 34 . )\\nwe are approving on an interim basis the replacement of the current windowing\\nprocess with a system under which socalgas will establish receipt point\\ncapacities , subject to daily revision , on the basis of the physical maximums\\nfor each receipt point under the operating conditions expected for that day .\\ncustomers and shippers will know the daily maximums because they will be\\nposted on socalgas \\x01 , gasselect system daily prior to the nomination\\ndeadlines . if , in the aggregate , customers nominate more than the physical\\ncapacity at any receipt point , gas will be scheduled based on the upstream\\npipeline \\x01 , s capacity rights system . for wheeler ridge , at which more than one\\nupstream pipeline delivers gas , the maximum daily physical capacity would be\\nallocated between upstream sources pro rata on the basis of the prior day \\x01 , s\\nscheduled deliveries from each source .\\nthis system eliminates the mystery in how pro - rations are made , provides\\ncontinuity in capacity rights between the interstate and intrastate systems\\nand provides flexibility for customers in nominating at the most\\ncost - effective receipt point on any given day . we recognize that it does not\\nprovide for long - term planning , but the alternative under the cs of paid - for\\nfirm receipt point rights for the term of the settlement has the disadvantage\\nof locking customers into a receipt point that may lose value over the term .\\nin this period of gas price volatility , we believe that the more flexible\\nplan is the right one .\\nthus , we direct socalgas to withdraw advice letter 2837 and file a new advice\\nletter within 10 business days implementing the proposed receipt point\\nphysical capacity system . this may be the same as the exemplary tariff filed\\nwith the is or updated as necessary pursuant to subsequent proceedings . this\\ntariff revision will be effective within 30 days after the filing unless\\nrejected by the energy division .\\nin r . 98 - 01 - 011 , pg & e and edison particularly complained about the\\nrestrictions at wheeler ridge . ( exh . 15 in r . 98 - 01 - 11 , pp . 7 - 9 ( pg & e\\nrebuttal to market conditions report ) , and exh . 8 in r . 98 - 01 - 011 , pp . 29 - 31 ,\\n( southern california edison , market conditions report ) . ) one response in the\\nis to these complaints is the establishment of a formal receipt point at\\nhector road for all customers , subject to wheeler ridge access fees and\\nsurcharges . its capacity will be 50 mmcfd or greater as long as there are\\nnominations of that volume and mojave pipeline company delivers that much in\\nresponse to those nominations . this provision should allow greater\\nflexibility for shippers and customers as well as leveling the playing field\\nbetween socalgas and others at this interconnection . we will support\\nsocalgas \\x01 , application to the federal energy regulatory commission for\\napproval of hector road as a formal delivery point by mojave .\\nel paso natural gas company objected strongly to the provision in the is for\\nautomatically expanding of wheeler ridge capacity while this was not an\\noption specifically mentioned in d . 99 - 07 - 015 , we do not choose to stand on\\nthat technicality to exclude it from consideration here . once a proceeding\\nis open to settlement , the dynamics of settlement talks may bring in matters\\noutside the delineated scope , as they have done here with regard to wheeler\\nridge expansion and , for instance , pooling . both proposals respond to\\nconcerns raised in r . 98 - 01 - 011 , ( see citations in text above as well as panel\\nhearing testimony of mr . benjamin c . campbell , pg & e , tr . pp . 267 - 268 , jan 19 ,\\n1999 ) ) and neither was specifically excluded from further consideration in\\nd . 99 - 07 - 015 . we therefore view them as within the scope of this proceeding .\\nto the extent that other receipt points are also viewed as constrained , we\\nwelcome evidence to that effect in a future proceeding , as well as proposals\\nfor criteria to determine when expansion should be applied for . by 100 mmcfd\\nif a certain number of curtailments occurred , and for automatically allowing\\nthe expenses of that expansion to be rolled into rates . we do not wish to\\napprove automatic rate increases for all ratepayers for a facility for which\\nonly some may have use , but we believe that developing criteria for expansion\\nof receipt points is useful . hector road may not entirely alleviate the\\nproblem of constraints on northern gas flowing to the south .\\ntherefore , we approve that portion of section iii of the is that sets forth\\ncriteria for expansion , but provide that upon the meeting of that criteria ,\\nsocalgas shall submit an application for an expansion of the receipt point\\ncapacity . that application shall be processed in the regular way , with the\\nissues of need in the context of the entire system and foreseeable market\\nconditions considered . moreover , rolled - in or incremental rates , allocation\\nof cost among classes and consequent rate design will remain open for\\ndecision in that proceeding .\\nthus , the modification to the is that we make is in the first sentence of the\\nfirst full paragraph on page 8 . the words \\x01 & apply to \\x01 8 should be inserted\\nafter \\x01 & socalgas will \\x01 8 . we specifically disapprove the is language in the\\nmiddle on page 8 beginning with the words \\x01 & this settlement \\x01 8 through the end\\nof the paragraph , and the concomitant language in appendix a setting the cost\\nat $ 12 million in 1999 dollars .\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jeffery fawcett / et & s / enron on 11 / 27 / 2000\\n08 : 45 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nfrom : jeff dasovich on 11 / 22 / 2000 06 : 28 pm\\nsent by : jeff dasovich\\nto : jeffery fawcett / et & s / enron @ enron , susan scott / et & s / enron @ enron\\ncc :\\nsubject : bad proposed decision\\nwell , this shows the direction in which the \" new \" commission is heading . the\\nvery good news , though , is that tw \\' s proposal was included in both\\nsettlements ( now that \\' s hedging ! ) . thus , the benefits to tw were preserved\\nunder both proposals . congratulations . that \\' s fantastic - - hard work that\\npaid off .\\nwe will of course express out extreme dissappointment with the pd , and point\\nout that this decision condemns california to a 20 th century infrastructure ,\\nwhen the state \\' s 21 st century economy demands much , much better .\\nwe should discuss . since the pd empowers the likes of norm and florio , it\\nwill be important to play very close attention to implementation of hector .\\nsorry to have to be the one to deliver the news . but we have a knack of\\nmaking lemonade out of lemons and we \\' ll do out best to do the same here ,\\nwhatever turns up at the end .\\nbest ,\\njeff\\n- - - - - forwarded by jeff dasovich / na / enron on 11 / 22 / 2000 05 : 53 pm - - - - -\\nmichael . alexander @ sce . com\\n11 / 22 / 2000 05 : 27 pm\\nto : paul _ amirault % sce @ sce . com , tomb @ crossborderenergy . com , burkee @ cts . com ,\\ncraigc @ calpine . com , rick . counihan @ greenmountain . com , jdasovic @ enron . com ,\\nmday @ gmssr . com , douglas . porter @ sce . com\\ncc : colin . cushnie @ sce . com , inggm @ sce . com\\nsubject :\\nthe pd in the gas restructuring is out . i have yet to read the whole\\nthing , but the title \" approval with modifications of the interim\\nsettlement . . . \" does not bode well . according to steve watson ( and i only\\nhave steve \\' s statement second hand ) , the decision reflects a fear that the\\ntiming is wrong in light of the current volatile gas price market .\\n( see attached file : proposed . doc )\\n- -\\nmichael s . alexander\\nsouthern california edison\\n626 - 302 - 2029\\n626 - 302 - 3254 ( fax )\\n- proposed . doc'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['num_words']< 10000].sort_values('num_words',ascending=False).loc[30233].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['num_words'] > 10000].index,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=  \"prajjwal1/bert-tiny\" \n",
    "lemma  = WordNetLemmatizer() # \n",
    "tokneizer = AutoTokenizer.from_pretrained(model_path,do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]',' ',text)\n",
    "    text = re.sub(r'\\n','',text )\n",
    "    text = re.sub(r'\\t','',text )\n",
    "    re.sub(r'\\s{2,}', ' ', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = tokneizer.tokenize(text)\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Text'] = df['Text'].apply(text_cleaning)\n",
    "df_clean['Label'] = df['Label'].apply(lambda x: 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go ju ##rong point crazy available bug ##is n ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok la ##r joking wi ##f u ##i</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry 2 w ##k ##ly com ##p win fa cup fin...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say early ho ##r u c already say</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah think go u ##f life around though</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34639</th>\n",
       "      <td>hello e ##ja ##cula ##te within minute ##pen #...</td>\n",
       "      <td>1</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34640</th>\n",
       "      <td>hello welcome gig ##ap ##har ##m ##lin ##ne sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34641</th>\n",
       "      <td>got earlier expected wrapped cautiously ##im #...</td>\n",
       "      <td>1</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34642</th>\n",
       "      <td>ready rock let man rise solitude show u societ...</td>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34643</th>\n",
       "      <td>learn last 5 10 time longer ##bed read pl ##od...</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34644 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label  num_words\n",
       "0      go ju ##rong point crazy available bug ##is n ...      0        111\n",
       "1                          ok la ##r joking wi ##f u ##i      0         29\n",
       "2      free entry 2 w ##k ##ly com ##p win fa cup fin...      1        155\n",
       "3                 u dun say early ho ##r u c already say      0         49\n",
       "4                  nah think go u ##f life around though      0         61\n",
       "...                                                  ...    ...        ...\n",
       "34639  hello e ##ja ##cula ##te within minute ##pen #...      1        892\n",
       "34640  hello welcome gig ##ap ##har ##m ##lin ##ne sh...      1        281\n",
       "34641  got earlier expected wrapped cautiously ##im #...      1        803\n",
       "34642  ready rock let man rise solitude show u societ...      1        317\n",
       "34643  learn last 5 10 time longer ##bed read pl ##od...      1         74\n",
       "\n",
       "[34644 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulize_token(text):\n",
    "    print(f\"Token {tokneizer.tokenize(text)}\")\n",
    "    print(f\"Token to ID: {tokneizer.convert_tokens_to_ids(tokneizer.tokenize(text))}\")\n",
    "    print(f\"Token to String: {tokneizer.convert_tokens_to_string(tokneizer.tokenize(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ['audrey', 'robertson', '#', '#', 'tr', '##a', '#', '#', 'ns', '#', '#', 'west', '#', '#', 'er', '##n', 'pipeline', 'company', '#', '#', 'em', '##a', '#', '#', 'il', 'address', 'audrey', 'robertson', 'en', '#', '#', 'ron', 'com', '71', '#', '#', '3', '85', '#', '#', '3', '58', '#', '#', '49', '71', '#', '#', '3', '64', '#', '#', '6', '255', '#', '#', '1', 'fa', '#', '#', 'x', '##a', '#', '#', 'tt', '##a', '#', '#', 'che', '##d', 'dated', 'plane', 'schedule', 'please', 'call', 'question', 'virginia', 'neill', '#', '#', 'nor', '#', '#', 'the', '##r', '#', '#', 'n', 'plain', 'natural', 'gas', 'co', 'omaha', '04', '#', '#', '31', '#', '#', 'tel', '#', '#', 'ep', '#', '#', 'hon', '##e', '402', '39', '#', '#', '8', '70', '#', '#', '7', '#', '#', '1', '#', '#', 'fa', '#', '#', 'x', '402', '39', '#', '#', '8', '75', '#', '#', '59']\n",
      "Token to ID: [14166, 9923, 1001, 1001, 19817, 2050, 1001, 1001, 24978, 1001, 1001, 2225, 1001, 1001, 9413, 2078, 13117, 2194, 1001, 1001, 7861, 2050, 1001, 1001, 6335, 4769, 14166, 9923, 4372, 1001, 1001, 6902, 4012, 6390, 1001, 1001, 1017, 5594, 1001, 1001, 1017, 5388, 1001, 1001, 4749, 6390, 1001, 1001, 1017, 4185, 1001, 1001, 1020, 20637, 1001, 1001, 1015, 6904, 1001, 1001, 1060, 2050, 1001, 1001, 23746, 2050, 1001, 1001, 18178, 2094, 6052, 4946, 6134, 3531, 2655, 3160, 3448, 11511, 1001, 1001, 4496, 1001, 1001, 1996, 2099, 1001, 1001, 1050, 5810, 3019, 3806, 2522, 12864, 5840, 1001, 1001, 2861, 1001, 1001, 10093, 1001, 1001, 4958, 1001, 1001, 10189, 2063, 28048, 4464, 1001, 1001, 1022, 3963, 1001, 1001, 1021, 1001, 1001, 1015, 1001, 1001, 6904, 1001, 1001, 1060, 28048, 4464, 1001, 1001, 1022, 4293, 1001, 1001, 5354]\n",
      "Token to String: audrey robertson # # tra # # ns # # west # # ern pipeline company # # ema # # il address audrey robertson en # # ron com 71 # # 3 85 # # 3 58 # # 49 71 # # 3 64 # # 6 255 # # 1 fa # # xa # # tta # # ched dated plane schedule please call question virginia neill # # nor # # ther # # n plain natural gas co omaha 04 # # 31 # # tel # # ep # # hone 402 39 # # 8 70 # # 7 # # 1 # # fa # # x 402 39 # # 8 75 # # 59\n"
     ]
    }
   ],
   "source": [
    "visulize_token(df_clean['Text'][30288])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train val test split 80/10/10\n",
    "train_input,_ = train_test_split(df_clean,test_size=.2)\n",
    "val_input,test_input = train_test_split(_,test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512 #max length support by bert\n",
    "def mask_input_for_bert(text_df,max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in text_df:\n",
    "        encoded_dict = tokneizer.encode_plus(\n",
    "            text = text,\n",
    "            add_special_tokens= True,\n",
    "            max_length=max_len,\n",
    "            padding = 'max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation= True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.tensor((input_ids))\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    return input_ids,attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inp,df_mask = mask_input_for_bert(df_clean['Text'],max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp,train_mask = mask_input_for_bert(train_input['Text'],max_len)\n",
    "valid_inp,valid_mask = mask_input_for_bert(val_input['Text'],max_len)\n",
    "test_inp,test_mask = mask_input_for_bert(test_input['Text'],max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = torch.tensor(train_input['Label'].values)\n",
    "valid_label =torch.tensor(val_input['Label'].values)\n",
    "test_label = torch.tensor(test_input['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_inp, train_mask, train_label)\n",
    "valid_dataset = TensorDataset(valid_inp, valid_mask, valid_label)\n",
    "test_dataset = TensorDataset(test_inp, test_mask, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a data loader\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training steps\n",
    "num_epochs = 50\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "# Optimizer\n",
    "# Update 05.2023 - Tring to use Lion optimizer introduced recently\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "#optimizer = optim.AdamW(params=model.parameters(), lr=1e-6)\n",
    "optimizer = Lion(model.parameters(), lr=1e-6, weight_decay=1e-2)\n",
    "# Set up the learning rate scheduler\n",
    "lr_scheduler = get_scheduler(name=\"linear\", \n",
    "                             optimizer=optimizer, \n",
    "                             num_warmup_steps=0, \n",
    "                             num_training_steps=num_training_steps)\n",
    "\n",
    "# Send model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric1 = evaluate.load(\"accuracy\")\n",
    "metric2 = evaluate.load(\"f1\")\n",
    "metric3 = evaluate.load(\"recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze top layer of model and change classifer \n",
    "for param in model.parameters():\n",
    "    param.required_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.classifier.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_features = model.classifier.in_features\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p = 0.25),\n",
    "    nn.Linear(in_features=num_features,out_features=2,bias = True)\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.3565599779997553, Valid Loss: 0.0046843787866018664, Valid Acc: 0.9041570438799076, Accuracy: {'accuracy': 0.9041570438799076}, F1: {'f1': 0.8873812754409769}, Recall: {'recall': 0.8977350720658888}\n",
      "Epoch 2, Train Loss: 0.654978871929206, Valid Loss: 0.003936202448145347, Valid Acc: 0.9174364896073903, Accuracy: {'accuracy': 0.9174364896073903}, F1: {'f1': 0.9017857142857142}, Recall: {'recall': 0.9011667810569663}\n",
      "Epoch 3, Train Loss: 0.9064466900745844, Valid Loss: 0.0034161973400493144, Valid Acc: 0.9292725173210161, Accuracy: {'accuracy': 0.9292725173210161}, F1: {'f1': 0.9170896785109983}, Recall: {'recall': 0.9299931365820179}\n",
      "Epoch 4, Train Loss: 1.1220821557815448, Valid Loss: 0.002964216518897658, Valid Acc: 0.9399538106235565, Accuracy: {'accuracy': 0.9399538106235565}, F1: {'f1': 0.9290102389078498}, Recall: {'recall': 0.9341111873713109}\n",
      "Epoch 5, Train Loss: 1.311237151326809, Valid Loss: 0.0026118824071321833, Valid Acc: 0.9477482678983834, Accuracy: {'accuracy': 0.9477482678983834}, F1: {'f1': 0.9381196581196581}, Recall: {'recall': 0.9416609471516816}\n",
      "Epoch 6, Train Loss: 1.4781990846112576, Valid Loss: 0.0023937421672338703, Valid Acc: 0.9523672055427251, Accuracy: {'accuracy': 0.9523672055427251}, F1: {'f1': 0.9436667804711505}, Recall: {'recall': 0.9485243651338366}\n",
      "Epoch 7, Train Loss: 1.6264133121955642, Valid Loss: 0.0022141618297089054, Valid Acc: 0.9549653579676675, Accuracy: {'accuracy': 0.9549653579676675}, F1: {'f1': 0.9464653397391901}, Recall: {'recall': 0.9464653397391901}\n",
      "Epoch 8, Train Loss: 1.7589533575660279, Valid Loss: 0.0020419915634751597, Valid Acc: 0.956986143187067, Accuracy: {'accuracy': 0.956986143187067}, F1: {'f1': 0.948955121616992}, Recall: {'recall': 0.9505833905284832}\n",
      "Epoch 9, Train Loss: 1.8805797356073553, Valid Loss: 0.0019076888811910056, Valid Acc: 0.960161662817552, Accuracy: {'accuracy': 0.960161662817552}, F1: {'f1': 0.9524137931034483}, Recall: {'recall': 0.9478380233356212}\n",
      "Epoch 10, Train Loss: 1.9906372236997782, Valid Loss: 0.001789983029113803, Valid Acc: 0.9618937644341802, Accuracy: {'accuracy': 0.9618937644341802}, F1: {'f1': 0.954514128187457}, Recall: {'recall': 0.9505833905284832}\n",
      "Epoch 11, Train Loss: 2.0941190277161894, Valid Loss: 0.0017099580447905746, Valid Acc: 0.9636258660508084, Accuracy: {'accuracy': 0.9636258660508084}, F1: {'f1': 0.9567010309278351}, Recall: {'recall': 0.9553877831159918}\n",
      "Epoch 12, Train Loss: 2.189093291986957, Valid Loss: 0.0016433325398274574, Valid Acc: 0.9639145496535797, Accuracy: {'accuracy': 0.9639145496535797}, F1: {'f1': 0.9571771154504968}, Recall: {'recall': 0.9588194921070693}\n",
      "Epoch 13, Train Loss: 2.277014755495837, Valid Loss: 0.001593406510584521, Valid Acc: 0.9673787528868361, Accuracy: {'accuracy': 0.9673787528868361}, F1: {'f1': 0.9611015490533562}, Recall: {'recall': 0.9581331503088538}\n",
      "Epoch 14, Train Loss: 2.358985293148008, Valid Loss: 0.0015238107221876432, Valid Acc: 0.9691108545034642, Accuracy: {'accuracy': 0.9691108545034642}, F1: {'f1': 0.9633184778882413}, Recall: {'recall': 0.9643102264927934}\n",
      "Epoch 15, Train Loss: 2.436482661933754, Valid Loss: 0.0014748335213725203, Valid Acc: 0.9708429561200924, Accuracy: {'accuracy': 0.9708429561200924}, F1: {'f1': 0.9653753856702091}, Recall: {'recall': 0.9663692518874399}\n",
      "Epoch 16, Train Loss: 2.5076758611844294, Valid Loss: 0.0014057968050568402, Valid Acc: 0.9734411085450346, Accuracy: {'accuracy': 0.9734411085450346}, F1: {'f1': 0.9683195592286501}, Recall: {'recall': 0.9649965682910089}\n",
      "Epoch 17, Train Loss: 2.579799985671888, Valid Loss: 0.001382031871864086, Valid Acc: 0.9734411085450346, Accuracy: {'accuracy': 0.9734411085450346}, F1: {'f1': 0.9683631361760661}, Recall: {'recall': 0.9663692518874399}\n",
      "Epoch 18, Train Loss: 2.6440108529901485, Valid Loss: 0.0013389603791900376, Valid Acc: 0.9740184757505773, Accuracy: {'accuracy': 0.9740184757505773}, F1: {'f1': 0.969050894085282}, Recall: {'recall': 0.9670555936856554}\n",
      "Epoch 19, Train Loss: 2.7057038617303193, Valid Loss: 0.001332178822929345, Valid Acc: 0.9745958429561201, Accuracy: {'accuracy': 0.9745958429561201}, F1: {'f1': 0.969842357779301}, Recall: {'recall': 0.9711736444749485}\n",
      "Epoch 20, Train Loss: 2.7626552078314983, Valid Loss: 0.0012828733653204374, Valid Acc: 0.9748845265588915, Accuracy: {'accuracy': 0.9748845265588915}, F1: {'f1': 0.970051635111876}, Recall: {'recall': 0.9670555936856554}\n",
      "Epoch 21, Train Loss: 2.8169940541528407, Valid Loss: 0.0012675565057143775, Valid Acc: 0.9754618937644342, Accuracy: {'accuracy': 0.9754618937644342}, F1: {'f1': 0.9707401032702238}, Recall: {'recall': 0.967741935483871}\n",
      "Epoch 22, Train Loss: 2.8685773732044093, Valid Loss: 0.0012375307700910776, Valid Acc: 0.9754618937644342, Accuracy: {'accuracy': 0.9754618937644342}, F1: {'f1': 0.9708004122294744}, Recall: {'recall': 0.9698009608785175}\n",
      "Epoch 23, Train Loss: 2.9192039493486406, Valid Loss: 0.0012421268254882004, Valid Acc: 0.9751732101616628, Accuracy: {'accuracy': 0.9751732101616628}, F1: {'f1': 0.9704264099037138}, Recall: {'recall': 0.9684282772820865}\n",
      "Epoch 24, Train Loss: 2.965918291315076, Valid Loss: 0.0012519458060391575, Valid Acc: 0.9754618937644342, Accuracy: {'accuracy': 0.9754618937644342}, F1: {'f1': 0.9708604730887899}, Recall: {'recall': 0.9718599862731641}\n",
      "Epoch 25, Train Loss: 3.0126904935945116, Valid Loss: 0.0013460705851798332, Valid Acc: 0.9751732101616628, Accuracy: {'accuracy': 0.9751732101616628}, F1: {'f1': 0.9705681040383299}, Recall: {'recall': 0.9732326698695951}\n",
      "Epoch 26, Train Loss: 3.056868557475962, Valid Loss: 0.001187900494914244, Valid Acc: 0.9766166281755196, Accuracy: {'accuracy': 0.9766166281755196}, F1: {'f1': 0.972097829831209}, Recall: {'recall': 0.9684282772820865}\n",
      "Epoch 27, Train Loss: 3.0991083177703556, Valid Loss: 0.0012003865447078185, Valid Acc: 0.976905311778291, Accuracy: {'accuracy': 0.976905311778291}, F1: {'f1': 0.9725274725274726}, Recall: {'recall': 0.9718599862731641}\n",
      "Epoch 28, Train Loss: 3.140105721841176, Valid Loss: 0.001189791125312069, Valid Acc: 0.9763279445727483, Accuracy: {'accuracy': 0.9763279445727483}, F1: {'f1': 0.9718599862731641}, Recall: {'recall': 0.9718599862731641}\n",
      "Epoch 29, Train Loss: 3.1800314125418234, Valid Loss: 0.0012558084134524365, Valid Acc: 0.9763279445727483, Accuracy: {'accuracy': 0.9763279445727483}, F1: {'f1': 0.9718599862731641}, Recall: {'recall': 0.9718599862731641}\n",
      "Epoch 30, Train Loss: 3.2181808063170276, Valid Loss: 0.0012202942737175998, Valid Acc: 0.9766166281755196, Accuracy: {'accuracy': 0.9766166281755196}, F1: {'f1': 0.9722507708119219}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 31, Train Loss: 3.2546624697987334, Valid Loss: 0.001208639294624234, Valid Acc: 0.976905311778291, Accuracy: {'accuracy': 0.976905311778291}, F1: {'f1': 0.9725839616175462}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 32, Train Loss: 3.2919237743387177, Valid Loss: 0.0011894157537857102, Valid Acc: 0.9766166281755196, Accuracy: {'accuracy': 0.9766166281755196}, F1: {'f1': 0.9721936148300722}, Recall: {'recall': 0.9718599862731641}\n",
      "Epoch 33, Train Loss: 3.3262527426028803, Valid Loss: 0.0011874099258563588, Valid Acc: 0.976905311778291, Accuracy: {'accuracy': 0.976905311778291}, F1: {'f1': 0.9725463280713795}, Recall: {'recall': 0.9725463280713795}\n",
      "Epoch 34, Train Loss: 3.359883268620746, Valid Loss: 0.001383927499517308, Valid Acc: 0.976905311778291, Accuracy: {'accuracy': 0.976905311778291}, F1: {'f1': 0.972565157750343}, Recall: {'recall': 0.9732326698695951}\n",
      "Epoch 35, Train Loss: 3.3917382032216703, Valid Loss: 0.00119883938709717, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9728801922416752}, Recall: {'recall': 0.9725463280713795}\n",
      "Epoch 36, Train Loss: 3.4237819169819663, Valid Loss: 0.0011853600184618136, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9728801922416752}, Recall: {'recall': 0.9725463280713795}\n",
      "Epoch 37, Train Loss: 3.454309372860114, Valid Loss: 0.0011890108103478892, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 38, Train Loss: 3.4851161476465, Valid Loss: 0.001207946570628854, Valid Acc: 0.976905311778291, Accuracy: {'accuracy': 0.976905311778291}, F1: {'f1': 0.9725839616175462}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 39, Train Loss: 3.5157646982188115, Valid Loss: 0.0012339509309239375, Valid Acc: 0.9777713625866051, Accuracy: {'accuracy': 0.9777713625866051}, F1: {'f1': 0.9735304228257132}, Recall: {'recall': 0.9718599862731641}\n",
      "Epoch 40, Train Loss: 3.5452449614866657, Valid Loss: 0.0013384209579076214, Valid Acc: 0.976905311778291, Accuracy: {'accuracy': 0.976905311778291}, F1: {'f1': 0.9725839616175462}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 41, Train Loss: 3.5753377065983116, Valid Loss: 0.0011879305116505325, Valid Acc: 0.9774826789838337, Accuracy: {'accuracy': 0.9774826789838337}, F1: {'f1': 0.9731958762886598}, Recall: {'recall': 0.9718599862731641}\n",
      "Epoch 42, Train Loss: 3.6039844474750935, Valid Loss: 0.001195845560008858, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 43, Train Loss: 3.6325277780935874, Valid Loss: 0.001334133922680567, Valid Acc: 0.9774826789838337, Accuracy: {'accuracy': 0.9774826789838337}, F1: {'f1': 0.9732510288065843}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 44, Train Loss: 3.662061582679414, Valid Loss: 0.0012106921974952413, Valid Acc: 0.9763279445727483, Accuracy: {'accuracy': 0.9763279445727483}, F1: {'f1': 0.9719178082191782}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 45, Train Loss: 3.692089218057774, Valid Loss: 0.0012042235821543594, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 46, Train Loss: 3.7215337906089667, Valid Loss: 0.0012007105882348402, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 47, Train Loss: 3.7510580556260096, Valid Loss: 0.0012008870589411355, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 48, Train Loss: 3.7791451643074088, Valid Loss: 0.0012010350730915405, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 49, Train Loss: 3.80842581436023, Valid Loss: 0.0012010317586155879, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n",
      "Epoch 50, Train Loss: 3.8369781971444517, Valid Loss: 0.0012009399688983331, Valid Acc: 0.9771939953810623, Accuracy: {'accuracy': 0.9771939953810623}, F1: {'f1': 0.9729173808707576}, Recall: {'recall': 0.9739190116678106}\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks)[0]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_losses.append(train_loss / len(train_loader))  # Calculate average train loss per epoch\n",
    "    \n",
    "    logits_alls = []\n",
    "    predicted_prob_all = []\n",
    "    predictions_all = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            attention_masks = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_masks)[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "            logits = outputs.logit\n",
    "            logits_alls.append(logits)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            valid_acc += torch.sum(preds == labels).item()\n",
    "            y_true += labels.cpu().numpy().tolist()\n",
    "            y_pred += preds.cpu().numpy().tolist()\n",
    "            metric1.add_batch(predictions=preds, references=batch[2])\n",
    "            metric2.add_batch(predictions=preds, references=batch[2])\n",
    "            metric3.add_batch(predictions=preds, references=batch[2])\n",
    "        \n",
    "        valid_loss /= len(valid_dataset)\n",
    "        valid_acc /= len(valid_dataset)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}, Valid Acc: {valid_acc}, Accuracy: {metric1.compute()}, F1: {metric2.compute()}, Recall: {metric3.compute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.0, Test Acc: 0.0, Accuracy {'accuracy': 0.9714285714285714}, F1 {'f1': 0.9657320872274143} Recall {'recall': 0.9680777238029147}\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_acc = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask)[0]\n",
    "            loss = criterion(outputs,labels)\n",
    "            valid_loss+=loss.item()\n",
    "            logits = outputs.logit\n",
    "            logits_alls.append(logits)\n",
    "            _,preds = torch.max(outputs,dim = 1)\n",
    "            valid_acc += torch.sum(preds == labels).item()\n",
    "            y_true += labels.cpu().numpy().tolist()\n",
    "            y_pred += preds.cpu().numpy().tolist()\n",
    "            metric1.add_batch(predictions=preds, references=batch[2])\n",
    "            metric2.add_batch(predictions=preds, references=batch[2])\n",
    "            metric3.add_batch(predictions=preds, references=batch[2])\n",
    "        test_loss /= len(test_dataset)\n",
    "        test_acc /= len(test_dataset)\n",
    "        print(f\"Test loss {test_loss}, Test Acc: {test_acc}, Accuracy {metric1.compute()}, F1 {metric2.compute()} Recall {metric3.compute()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model and parameterfor loading at backend\n",
    "torch.save(model,'model_presentation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST BY INPUTING RAW STRING\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "device\n",
    "model = torch.load('model_presentation.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(text):\n",
    "    inputs_input_ids = torch.tensor(tokneizer.encode(text, add_special_tokens=True)).unsqueeze(0).to(device)\n",
    "    logits = model(inputs_input_ids)[0].to(device)\n",
    "    # Convert the logits to probabilities\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0)\n",
    "    # Get the predicted label and its probability\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    pred_prob = probs[pred_label]\n",
    "    return pred_label,pred_prob.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"URGENT: Your bank account has been compromised! Click the link below to verify your account details immediately, or your funds may be at risk.\"\n",
    "text2 = 'Hey Mark, long time no see! I heard you recently onduct of tax claim for last financial year, call below number now to avoid penalty,'\n",
    "text10= 'Hi Alex, john gave me this number for the event tomorrow, can i call you and discuss some detail when you are free? Thanks!'\n",
    "text11 = 'Hey josh, I knew someone yesterday saying he got some way to grab some quick cash, i know you are kinda short on money so count you in?'\n",
    "text12= 'Hi Josh, need some quick cash? callmoved to the city. How about we grab dinner this weekend and catch up? I know a great restaurant downtown. Let me know if youre interested. Cheers, Lisa.'\n",
    "text3 = \"Attention: Your Amazon account has been temporarily suspended due to suspicious activity. To reactivate your account and secure your personal information, please click the link below and provide your login credentials and updated payment details within 24 hours. Failure to comply may result in permanent account termination.\"\n",
    "text4 = 'reset your password now!, your netflex account has been compromised'\n",
    "text5 = 'wanna earn some fast cash? click the link below and find out more, totally legal and legit, no tricks just win'\n",
    "text6 = 'Call me when you got home okay? Hope you had a good night'\n",
    "text7 = 'Hello, your shipment from UPS will arrive today. Click here to track your package'\n",
    "text8 = '“Your Wells Fargo account has been locked for suspicious activity. Please log in here and verify your account.”'\n",
    "text9 = 'This is ATO trying to reach you out regarding a msc us today, no tricks, just money'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string is idenfity as spam with probability of 0.73195\n"
     ]
    }
   ],
   "source": [
    "label,probability = test_model(text7)#Testing model using various of new strings\n",
    "print(f\"This string is idenfity as {'spam' if label == 1 else 'ham'} with probability of {round(probability,5)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for futrue improvement\n",
    "- Retrain model using newly seen spam sms. As this might related to more recent topic, ie COVID, loan interest\n",
    "- Build a ETL pipeline to automatelly apply preprocessing to raw data, feed clean data to model directly for training (Archiecture: AWS - MAGE - SAGEMAKER or equvient)\n",
    "- tring different "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
